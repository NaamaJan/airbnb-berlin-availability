{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Feature Enrichment \n",
    "\n",
    "There are 3 ways to enrich the data:\n",
    "1. Feature Extraction: obtaining new features from existing features.\n",
    "2. Feature Engineering: transformation of raw data into features suitable for modeling.\n",
    "3. Feature Transformation: transformation of data to improve the accuracy of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"flat_file_after_data_cleansing.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding the output display to see more rows and columns:\n",
    "pd.set_option('display.max_rows', 200 , 'display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_additional_index_columns(_df):\n",
    "    # dropping additional index columns that start with \"Unnamed\" - dropping these columns\n",
    "    columns_to_drop = [x for x in _df.columns.to_list() if x.startswith(\"Unnamed\")]\n",
    "    print(\"dropping coulmns: \", columns_to_drop) # [Unamed..., Unamed..]\n",
    "    return _df.drop(columns=columns_to_drop, axis=1, inplace=False)\n",
    "\n",
    "df = drop_additional_index_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing the dimensionality of the DataFrame (before adding new variables):\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "The addtional variables that were created in \"Addition to Flat file\" notebook are:\n",
    "1. size - extract it from the \"description\" column.\n",
    "2. concat_comments_polarity (Sentiment Analysis) - extract from \"concat_comments\" column\n",
    "3. concat_comments_subjectivity (Sentiment Analysis) - extract from \"concat_comments\" column\n",
    "4. concat_comments_sentiment  (Sentiment Analysis) - extract from \"concat_comments\" column\n",
    "\n",
    "This variables were created in the Addition to Flat file notebook because these include NA and need to be handled in the EDA and in the Data Cleansing section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature Engineering is based on the patterns from EDA section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable modification\n",
    "\n",
    "I have already performed transformation in the Data cleansing section (fixing outliers).\n",
    "\n",
    "\n",
    "The following variables have been transfomed (because could not replaced with None): \n",
    "1. 'target_avg_dollar_price_in_period' --> 'log_target_avg_dollar_price_in_period'\n",
    "2. 'calculated_host_listings_count' --> 'sqrt_calculated_host_listings' and 'sigmoid_calculated_host_listings_count'\n",
    "3. 'bedrooms' --> 'sqrt_bedrooms'\n",
    "4. 'guests_included' --> 'sqrt_guests_included'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of two or more variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   1. Numeric Columns Correlations\n",
    "Checking the correlation between the variables. \n",
    "If the correlation between the variables is high (more than 0.8 or less than -0.8) I will need to choose one of them\n",
    "(the one with the higher correlation to the target variable), and create new variables that represents the transformation of them and drop the other variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda_numric_correlations = pd.read_csv(\"eda_numric_correlations.csv\")\n",
    "# This table has been taken from the EDA section, representing the variables that have a high correlation \n",
    "# (more than 0.8 or less than -0.8) based on Spearman's rank correlation coefficient test.\n",
    "df_eda_numric_correlations = drop_additional_index_columns(df_eda_numric_correlations)\n",
    "df_eda_numric_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.a Enrich with cluster analysis\n",
    "Checking the correlation between numeric variables (eda_numric_correlations.csv).\n",
    "There is high correlation between ['availability_30','availability_60', 'availability_90', 'availability_365'], using cluster analysis creating new var which can replace this vars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability_columns = ['availability_30','availability_60', 'availability_90', 'availability_365']\n",
    "\n",
    "df_eda_numric_correlations[(df_eda_numric_correlations['var1'].isin(availability_columns)) & \n",
    "                           (df_eda_numric_correlations['var2'].isin(availability_columns))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_elbow_of_selected_columns(X, k_min_range=1, k_max_range=10):\n",
    "    \"\"\"\n",
    "    Using plot of the Elbow Method to determine this optimal value of k.\n",
    "    This method is only for plotting. Base on the plot the user need to select k.\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_columns = X.columns\n",
    "    display(X.head(1))\n",
    "\n",
    "    distortions = []\n",
    "    K = range(k_min_range,k_max_range)\n",
    "    for k in K:\n",
    "        kmeanModel = KMeans(n_clusters=k)\n",
    "        kmeanModel.fit(X)\n",
    "        distortions.append(kmeanModel.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title(f'Optimal k analysis of columns: {str(selected_columns)}' )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting kmeanModel elbow for selecting K\n",
    "X = df[availability_columns]\n",
    "display_elbow_of_selected_columns(X,  1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on the above plot selecting n_clusters=3 \n",
    "kmodel = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "print(\"kmodel.labels_:\", kmodel.labels_)\n",
    "print(\"np.bincount(kmodel.labels_):\", np.bincount(kmodel.labels_))\n",
    "sns.scatterplot(df['availability_30'], df['availability_365'],hue= kmodel.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above it seems there are significant clusters sepration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmean_cluster_availability']= kmodel.labels_\n",
    "chi2_res = stats.chi2_contingency(pd.crosstab(df['kmean_cluster_availability'], df['booked_up_target']))\n",
    "print(\"1a. pvalue of kmean_cluster_availability and booked_up_target is: \", chi2_res[1])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'kmean_cluster_availability' has pvalue < 0.05 --> keep it as feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b  Transformation of vars with subtraction and division\n",
    "\n",
    "Checking the correlation between the variables (eda_numric_correlations.csv).\n",
    "If the correlation between 2 variables is high (more than 0.8 or less than -0.8) creating new variables of division and subtraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda_numric_correlations[np.abs(df_eda_numric_correlations['correlation_between_vars'])>=0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding subtraction and division variables of two variables with high correlation\n",
    "# before dropping the variable with the lowest correlation with the target\n",
    "def add_minus_and_div_to_numric_correlations_columns(_df:pd.DataFrame ,_df_numric_correlations: pd.DataFrame, correlation_between_vars_threshold: float = 0.8):\n",
    "    \"\"\"\n",
    "    For each 2 numeric columns which have high corrleation between them, creating 2 new vars/fatures:\n",
    "    1. var1-var2\n",
    "    2. var1/var2\n",
    "    \"\"\"\n",
    "    if correlation_between_vars_threshold:\n",
    "        df_numric_correlations_filter_by_corr = _df_numric_correlations[np.abs(_df_numric_correlations['correlation_between_vars'])>=correlation_between_vars_threshold]\n",
    "    else:\n",
    "        df_numric_correlations_filter_by_corr = _df_numric_correlations\n",
    "        \n",
    "    for idx, row in df_numric_correlations_filter_by_corr.iterrows():\n",
    "        var1 = row['var1']\n",
    "        var2 = row['var2']\n",
    "        minus_column_name = var1 + \"_minus_\" + var2\n",
    "        div_column_name = var1 + \"_div_\" + var2\n",
    "        _df[minus_column_name] = _df[var1] - _df[var2]\n",
    "        _df[div_column_name] = _df[var1] / _df[var2]\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation on each 2 columns (create minus and div for each couple of numeric columns)\n",
    "df = add_minus_and_div_to_numric_correlations_columns(_df=df,_df_numric_correlations=df_eda_numric_correlations, correlation_between_vars_threshold=0.8 )\n",
    "print(\"1b. transformation new colums:\")\n",
    "transform_col_list  = [col for col in df.columns if \"_minus_\" in col or \"_div_\" in col]\n",
    "print(\"transformation new colums:\", transform_col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.c  Get numric columns that can be dropped by high correlation\n",
    "\n",
    "Checking the correlation between the variables (eda_numric_correlations.csv).\n",
    "If the correlation between 2 variables is high (more than 0.8 or less than -0.8) check which variable has the highest and the lowest correlation with the target and keeping only the var that has highest correlation with the target.  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_numeric_cols_to_drop_by_correlation(_df_numric_correlations, correlation_between_vars_threshold = 0.8):\n",
    "    \"\"\"\n",
    "    For each 2 numeric columns which have high corrleation between them (more then 80%), \n",
    "    check which variable has the highest and the lowest correlation with the target, \n",
    "    keep the var with highest correlation to the target and get a list of drop vars which has lowest correlation with target\n",
    "    _df_numric_correlations has the following columns\n",
    "    ['var1', 'var2', 'target', 'correlation_between_vars',\n",
    "           'pvalue_between_vars', 'correlation_var1_target', 'pvalue_var1_target',\n",
    "           'correlation_var2_target', 'pvalue_var2_target']\n",
    "    \"\"\"\n",
    "    # get rows that 2 vars/features have high correlation between them. \n",
    "    df_numric_correlations_filter_by_corr = _df_numric_correlations[np.abs(_df_numric_correlations['correlation_between_vars'])>=correlation_between_vars_threshold]\n",
    "    \n",
    "    columns_to_drop = set()\n",
    "    # for each row we drop the var that has low correlation with target column\n",
    "    for idx, row in df_numric_correlations_filter_by_corr.iterrows():\n",
    "        correlation_var1_target = row['correlation_var1_target']\n",
    "        correlation_var2_target = row['correlation_var2_target']\n",
    "        if correlation_var1_target and correlation_var2_target and np.abs(correlation_var1_target) > np.abs(correlation_var2_target):\n",
    "            columns_to_drop.add(row['var2'])\n",
    "        else:\n",
    "            columns_to_drop.add(row['var1'])\n",
    "    return columns_to_drop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of numeric columns to drop (for each couple of vars drop the columns with lowest correlation to target)\n",
    "numeric_cols_to_drop = get_numeric_cols_to_drop_by_correlation(df_eda_numric_correlations)\n",
    "print(\"1c. dropping colums:\", numeric_cols_to_drop)\n",
    "\n",
    "df.drop(numeric_cols_to_drop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explantion of the reuslts:\n",
    "1. 'availability_30', 'availability_60', 'availability_90' and 'availability_365' have high correlation between each of them (correlatioin is more than  80%). The fucntion keeps only 'availability_365' var because this var has the highest correlation with the target. \n",
    "2. \"target_avg_dollar_price_in_period\", \"avg_dollar_price_in_previous_period\", \"weekly_price\" and \"price\" have high correlatioin between each of them but \"price\" has the highest correlation to target. \n",
    "Also, monthly_price is not dropped because it doesn't have correlation with price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Categorical Columns Statistically Difference\n",
    "\n",
    "Using chi-squared test to determine whether there is a statistically significant difference between the two categories variables.\n",
    "\n",
    "This chi-squared test assist finding new variables.\n",
    "I am going to explore if there are combinations between the values of the categories variables that are statistically significant to the target \"booked_up_target\", and adding them as variables to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda_pval_of_categorical_columns = pd.read_csv(\"eda_pval_of_categorical_columns.csv\")\n",
    "# This table has been taken from the EDA section, representing the categorical variables \n",
    "# that have statistically significant differences,  based on  Chi-squared Test (χ2 test).\n",
    "df_eda_pval_of_categorical_columns = drop_additional_index_columns(df_eda_pval_of_categorical_columns)\n",
    "df_eda_pval_of_categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obsolute method (not in use any more, keeping it becuase of the test)\n",
    "def get_dummies_of_2_vars(_df, _col1, _col2, unique_values_limitation=7):\n",
    "    \"\"\"\n",
    "    \"create dummies\" of 2 categorical colums -> create new column for each possibale metch of value. \n",
    "    \"\"\"\n",
    "    col1_unique_values = _df[_col1].unique()\n",
    "    col2_unique_values = _df[_col2].unique()\n",
    "    col1_unique_count = len(col1_unique_values)\n",
    "    col2_unique_count = len(col2_unique_values)\n",
    "    if unique_values_limitation != None:\n",
    "        if  col1_unique_count > unique_values_limitation or col2_unique_count > unique_values_limitation:\n",
    "            print(f' *** {_col1} has {col1_unique_count} unique values and {_col2} has {col2_unique_count} unique values - Not creating dummies ****')\n",
    "            return _df\n",
    "    print(_col1, \"number of unique values: \", len(col1_unique_values))\n",
    "    print(_col2, \"number of unique values: \", len(col2_unique_values))\n",
    "    added_columns = []\n",
    "    def _is_dummy(_row_val_1,_unique_val1,_row_val_2 ,_unique_val2):\n",
    "        if _row_val_1 == _unique_val1 and _row_val_2 == _unique_val2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    for unique_val_1 in col1_unique_values:\n",
    "        for unique_val_2 in col2_unique_values:\n",
    "            \n",
    "            print(\"_col1:\", _col1 ,\"unique_val_1: \", unique_val_1,\"_col2:\",_col2, \"unique_val_2: \", unique_val_2)\n",
    "            new_col = \"_\".join([_col1, unique_val_1, _col2, unique_val_2])\n",
    "            added_columns.append(new_col)\n",
    "            print(\"new_col:\", new_col)\n",
    "            _df[new_col] = _df.apply(lambda row:_is_dummy(row[_col1], unique_val_1, row[_col2], unique_val_2),axis=1)\n",
    "    print(\"number of added columns\", len(added_columns))\n",
    "    return _df\n",
    "\n",
    "###############################################################################################\n",
    "#### Test\n",
    "###############################################################################################\n",
    "df_test = pd.DataFrame({\"my_col_1\":[\"a1\", \"a2\", \"a1\",\"a1\"], \"my_col_2\": [\"b1\", \"b1\", \"b2\", \"b2\"]})\n",
    "df_actual_test = get_dummies_of_2_vars(df_test, 'my_col_1', \"my_col_2\")\n",
    "display(df_actual_test)\n",
    "df_expected_test = pd.DataFrame({\"my_col_1\":[\"a1\", \"a2\", \"a1\",\"a1\"], \"my_col_2\": [\"b1\", \"b1\", \"b2\", \"b2\"], \n",
    "                           \"my_col_1_a1_my_col_2_b1\": [1, 0 , 0 ,0],\n",
    "                           \"my_col_1_a1_my_col_2_b2\": [0, 0, 1 , 1],\n",
    "                           \"my_col_1_a2_my_col_2_b1\": [0, 1, 0, 0],\n",
    "                           \"my_col_1_a2_my_col_2_b2\": [0, 0, 0, 0]})\n",
    "\n",
    "pd.testing.assert_frame_equal(df_actual_test,df_expected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies_of_2_vars_v2(_df, _col1, _col2,_target_col=\"booked_up_target\", unique_values_limitation=5, pval_threshold_of_new_col_with_target=0.05):\n",
    "    \"\"\"\n",
    "    \"create dummies\" of 2 categorical colums -> create new column for each possibale metch of value. \n",
    "    \"\"\"\n",
    "    col1_unique_values = _df[_col1].unique()\n",
    "    col2_unique_values = _df[_col2].unique()\n",
    "    col1_unique_count = len(col1_unique_values)\n",
    "    col2_unique_count = len(col2_unique_values)\n",
    "    added_columns = []\n",
    "    if unique_values_limitation != None:\n",
    "        if  col1_unique_count > unique_values_limitation or col2_unique_count > unique_values_limitation:\n",
    "            print(f' *** {_col1} has {col1_unique_count} unique values and {_col2} has {col2_unique_count} unique values - Not creating dummies ****')\n",
    "            return _df, []\n",
    "    print(_col1, \"number of unique values: \", len(col1_unique_values))\n",
    "    print(_col2, \"number of unique values: \", len(col2_unique_values))\n",
    "    \n",
    "    def _is_dummy(_row_val_1,_unique_val1,_row_val_2 ,_unique_val2):\n",
    "        if _row_val_1 == _unique_val1 and _row_val_2 == _unique_val2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    for unique_val_1 in col1_unique_values:\n",
    "        for unique_val_2 in col2_unique_values:\n",
    "            \n",
    "            print(\"_col1:\", _col1 ,\"unique_val_1: \", unique_val_1,\"_col2:\",_col2, \"unique_val_2: \", unique_val_2)\n",
    "            new_col = \"_\".join([_col1, unique_val_1, _col2, unique_val_2])\n",
    "            added_columns.append(new_col)\n",
    "            print(\"new_col:\", new_col)\n",
    "            _df[new_col] = _df.apply(lambda row:_is_dummy(row[_col1], unique_val_1, row[_col2], unique_val_2),axis=1)\n",
    "            chi2_res = stats.chi2_contingency(pd.crosstab(_df[new_col],_df[_target_col]))\n",
    "            pval = chi2_res[1]\n",
    "            if pval > pval_threshold_of_new_col_with_target:\n",
    "                print(f\"****chi2_contingency between {new_col} and {_target_col} have pval of {pval} (more than {pval_threshold_of_new_col_with_target})\")\n",
    "                print(f\"***drop {new_col} \")\n",
    "                _df.drop(new_col, inplace=True, axis=1)\n",
    "            else:\n",
    "                added_columns.append(new_col)\n",
    "    print(\"number of added columns\", len(added_columns))\n",
    "    return _df, added_columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda_pval_of_categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "total_columns = []\n",
    "num_of_uniqiue_values = {}\n",
    "for index, row in df_eda_pval_of_categorical_columns.iterrows():\n",
    "    start_time = datetime.now()\n",
    "    var1 = row['var1']\n",
    "    var2 = row['var2']\n",
    "    df[var1] = df[var1].astype('category')\n",
    "    df[var2] = df[var2].astype('category')\n",
    "    print(\"var1: \", var1)\n",
    "    print(\"var2: \", var2)\n",
    "    # _df, _col1, _col2,_target_col=\"booked_up_target\", unique_values_limitation=5, pval_threshold_of_new_col_with_target=0.05\n",
    "    df, added_columns = get_dummies_of_2_vars_v2(_df=df, _col1=var1, _col2=var2, _target_col=\"booked_up_target\", unique_values_limitation=5, pval_threshold_of_new_col_with_target=0.05)\n",
    "    total_columns + added_columns\n",
    "    end_time = datetime.now()\n",
    "    print(\"added_columns: \", added_columns)\n",
    "    print(row['var1'], row['var2'], \"total time: \", end_time-start_time)\n",
    "print(\"total_columns added: \", total_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich with external data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Enrich with distance from center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create column which indicates distance from center based on latitude and longitude variables.\n",
    "\n",
    "#Get Berlin coordinates\n",
    "#https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n",
    "#https://www.latlong.net/place/berlin-germany-9966.html\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "def distance_from_berlin(lat, lon):\n",
    "    berlin_centre = (52.50277, 13.404166)\n",
    "    record = (lat, lon)\n",
    "    return great_circle(berlin_centre, record).km\n",
    "\n",
    "#add distanse dataset\n",
    "df['distance_from_center'] = df.apply(lambda x: distance_from_berlin(x.latitude, x.longitude), axis=1)\n",
    "\n",
    "\n",
    "df['distance_from_center'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Enrich with  Average temperatures, Average precipitation and Sunshine hours\n",
    "\n",
    "source https://www.climatestotravel.com/climate/germany/berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average_temperatures = pd.read_csv(\"climatestotravel Berlin - Average temperatures.csv\")\n",
    "df_average_precipitation = pd.read_csv(\"climatestotravel Berlin - Average precipitation.csv\")\n",
    "df_sunshine_hours = pd.read_csv(\"climatestotravel Berlin - Sunshine hours.csv\")\n",
    "\n",
    "print(\"climatestotravel Berlin - Average temperatures:\")\n",
    "print(\"-----------------------------------------------\")\n",
    "display(df_average_temperatures)\n",
    "\n",
    "print(\"climatestotravel Berlin - Average precipitation:\")\n",
    "print(\"-----------------------------------------------\")\n",
    "display(df_average_precipitation)\n",
    "\n",
    "print(\"climatestotravel Berlin - Sunshine hours:\")\n",
    "print(\"-----------------------------------------------\")\n",
    "display(df_sunshine_hours)\n",
    "\n",
    "d = {'January':1, 'February':2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7, 'August':8, 'September':9, 'October':10, 'November':11, 'December':12}\n",
    "\n",
    "# Month\tMin (°C)\tMax (°C)\tMean (°C)\tMin (°F)\tMax (°F)\tMean (°F)\n",
    "df_average_temperatures = df_average_temperatures.rename(columns={'Month': 'month_name', 'Min (°C)': 'min_temperatures', 'Max (°C)': \"max_temperatures\",  'Mean (°C)': \"mean_temperatures\"})\n",
    "df_average_temperatures['month'] = df_average_temperatures.month_name.map(d)\n",
    "\n",
    "df_average_temperatures = df_average_temperatures[[\"month\",\"month_name\",\"min_temperatures\", \"max_temperatures\", \"mean_temperatures\"]]\n",
    "\n",
    "# Month\tMillimeters\tInches\tDays\n",
    "df_average_precipitation = df_average_precipitation.rename(columns={'Month': 'month_name', 'Millimeters': 'precipitation_millimeters', 'Days': \"precipitation_days\"})\n",
    "df_average_precipitation['month'] = df_average_precipitation.month_name.map(d)\n",
    "\n",
    "df_average_precipitation = df_average_precipitation[['month','month_name','precipitation_millimeters','precipitation_days']]\n",
    "# Month\tAverage\tTotal\n",
    "df_sunshine_hours = df_sunshine_hours.rename(columns={'Month': 'month_name','Average':'average_sunshine_hours_in_day', 'Total': 'average_sunshine_hours_in_month' })\n",
    "df_sunshine_hours['month'] = df_sunshine_hours.month_name.map(d)\n",
    "df_sunshine_hours = df_sunshine_hours[[\"month\",'month_name', 'average_sunshine_hours_in_day', 'average_sunshine_hours_in_month']]\n",
    "\n",
    "df_climatestotravel = df_average_temperatures.merge(df_average_precipitation).merge(df_sunshine_hours)\n",
    "# dropping year row\n",
    "df_climatestotravel = df_climatestotravel.drop(df_climatestotravel[df_climatestotravel['month_name'] == \"Year\"].index, inplace=False)\n",
    "df_climatestotravel['month'] = df_climatestotravel['month'].astype(int)\n",
    "display(df_climatestotravel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(start=\"2019-06-01\", end=\"2019-08-31\", freq=\"M\").month.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_from_climatestotravel_in_period(start_date_period, end_date_period, climatestotravel_selected_col):\n",
    "    \"\"\"\n",
    "    Using df_climatestotravel calculate the mean between range of dates\n",
    "    \"\"\"\n",
    "    # get list of months between dates, for example: \n",
    "    # pd.date_range(start=\"2019-06-01\", end=\"2019-08-31\", freq=\"M\").month.to_list() Getting [6, 7, 8]\n",
    "    list_of_months_between_dates = pd.date_range(start=start_date_period, end=end_date_period, freq=\"M\").month.to_list()\n",
    "    # filter the months in period\n",
    "    df_climatestotravel_selected_months = df_climatestotravel[df_climatestotravel['month'].isin(list_of_months_between_dates)]\n",
    "    # avergae\n",
    "    return df_climatestotravel_selected_months[climatestotravel_selected_col].mean()\n",
    "\n",
    "\n",
    "########################################################################\n",
    "## Test get_mean_from_climatestotravel_in_period\n",
    "########################################################################\n",
    "df_actual_test = pd.DataFrame({\"target_start_date_period\":[\"2019-06-01\", \"2019-02-01\"], \"target_end_date_period\": [\"2019-08-31\", \"2019-08-31\"]})\n",
    "\n",
    "df_actual_test['mean_temperatures_in_target_period'] = df_actual_test.apply(lambda row:get_mean_from_climatestotravel_in_period(start_date_period = row['target_start_date_period'], end_date_period = row['target_end_date_period'], climatestotravel_selected_col = \"mean_temperatures\"),axis=1)\n",
    "print(\"df_actual_test:\")\n",
    "display(df_actual_test)\n",
    "df_expected_test = pd.DataFrame({\"target_start_date_period\":[\"2019-06-01\", \"2019-02-01\"], \n",
    "                            \"target_end_date_period\": [\"2019-08-31\", \"2019-08-31\"],\n",
    "                           \"mean_temperatures_in_target_period\": [(17.0+19.0+19.0)/3.0, (1.0+5.0+8.5+14.0+17.0+19.0+19.0)/7.0]})\n",
    "\n",
    "print(\"df_expected_test:\")\n",
    "display(df_expected_test)\n",
    "pd.testing.assert_frame_equal(df_actual_test, df_expected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climatestotravel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print(start_time, \"mean_temperatures_in_previous_period start\")\n",
    "df['mean_temperatures_in_previous_period'] = df.apply(lambda row:\n",
    "                                                      get_mean_from_climatestotravel_in_period(start_date_period = row['start_date_previous_period'], \n",
    "                                                                                               end_date_period = row['end_date_previous_period'], \n",
    "                                                                                               climatestotravel_selected_col = \"mean_temperatures\"),axis=1)\n",
    "print(datetime.datetime.now(), \"mean_temperatures_in_previous_period total time : \", datetime.datetime.now() - start_time)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print(start_time, \"mean_temperatures_in_target_period start\")\n",
    "\n",
    "df['mean_temperatures_in_target_period'] = df.apply(lambda row:\n",
    "                                                    get_mean_from_climatestotravel_in_period(start_date_period = row['target_start_date_period'], \n",
    "                                                                                             end_date_period = row['target_end_date_period'], \n",
    "                                                                                             climatestotravel_selected_col = \"mean_temperatures\"),axis=1)\n",
    "\n",
    "print(datetime.datetime.now(), \"mean_temperatures_in_target_period total time : \", datetime.datetime.now() - start_time)\n",
    "\n",
    "df['mean_of_min_temperatures_in_previous_period'] = df.apply(lambda row:\n",
    "                                                      get_mean_from_climatestotravel_in_period(start_date_period = row['start_date_previous_period'], \n",
    "                                                                                               end_date_period = row['end_date_previous_period'], \n",
    "                                                                                               climatestotravel_selected_col = \"min_temperatures\"),axis=1)\n",
    "\n",
    "df['mean_of_min_temperatures_in_target_period'] = df.apply(lambda row:\n",
    "                                                    get_mean_from_climatestotravel_in_period(start_date_period = row['target_start_date_period'], \n",
    "                                                                                             end_date_period = row['target_end_date_period'], \n",
    "                                                                                             climatestotravel_selected_col = \"min_temperatures\"),axis=1)\n",
    "\n",
    "df['mean_of_max_temperatures_in_previous_period'] = df.apply(lambda row:\n",
    "                                                      get_mean_from_climatestotravel_in_period(start_date_period = row['start_date_previous_period'], \n",
    "                                                                                               end_date_period = row['end_date_previous_period'], \n",
    "                                                                                               climatestotravel_selected_col = \"max_temperatures\"),axis=1)\n",
    "\n",
    "df['mean_of_max_temperatures_in_target_period'] = df.apply(lambda row:\n",
    "                                                    get_mean_from_climatestotravel_in_period(start_date_period = row['target_start_date_period'], \n",
    "                                                                                             end_date_period = row['target_end_date_period'], \n",
    "                                                                                             climatestotravel_selected_col = \"max_temperatures\"),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df['mean_precipitation_millimeters_in_previous_period'] = df.apply(lambda row:\n",
    "                                                      get_mean_from_climatestotravel_in_period(start_date_period = row['start_date_previous_period'], \n",
    "                                                                                               end_date_period = row['end_date_previous_period'], \n",
    "                                                                                               climatestotravel_selected_col = \"precipitation_millimeters\"),axis=1)\n",
    "\n",
    "df['mean_precipitation_millimeters_in_target_period'] = df.apply(lambda row:\n",
    "                                                    get_mean_from_climatestotravel_in_period(start_date_period = row['target_start_date_period'], \n",
    "                                                                                             end_date_period = row['target_end_date_period'], \n",
    "                                                                                             climatestotravel_selected_col = \"precipitation_millimeters\"),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df['mean_precipitation_days_in_previous_period'] = df.apply(lambda row:\n",
    "                                                      get_mean_from_climatestotravel_in_period(start_date_period = row['start_date_previous_period'], \n",
    "                                                                                               end_date_period = row['end_date_previous_period'], \n",
    "                                                                                               climatestotravel_selected_col = \"precipitation_days\"),axis=1)\n",
    "\n",
    "df['mean_precipitation_days_in_target_period'] = df.apply(lambda row:\n",
    "                                                    get_mean_from_climatestotravel_in_period(start_date_period = row['target_start_date_period'], \n",
    "                                                                                             end_date_period = row['target_end_date_period'], \n",
    "                                                                                             climatestotravel_selected_col = \"precipitation_days\"),axis=1)\n",
    "\n",
    "\n",
    "df['mean_sunshine_hours_in_day_in_previous_period'] = df.apply(lambda row:\n",
    "                                                      get_mean_from_climatestotravel_in_period(start_date_period = row['start_date_previous_period'], \n",
    "                                                                                               end_date_period = row['end_date_previous_period'], \n",
    "                                                                                               climatestotravel_selected_col = \"average_sunshine_hours_in_day\"),axis=1)\n",
    "\n",
    "df['mean_sunshine_hours_in_day_in_target_period'] = df.apply(lambda row:\n",
    "                                                    get_mean_from_climatestotravel_in_period(start_date_period = row['target_start_date_period'], \n",
    "                                                                                             end_date_period = row['target_end_date_period'], \n",
    "                                                                                             climatestotravel_selected_col = \"average_sunshine_hours_in_day\"),axis=1)\n",
    "\n",
    "\n",
    "df['mean_sunshine_hours_in_month_in_previous_period'] = df.apply(lambda row:\n",
    "                                                      get_mean_from_climatestotravel_in_period(start_date_period = row['start_date_previous_period'], \n",
    "                                                                                               end_date_period = row['end_date_previous_period'], \n",
    "                                                                                               climatestotravel_selected_col = \"average_sunshine_hours_in_month\"),axis=1)\n",
    "\n",
    "df['mean_sunshine_hours_in_month_in_target_period'] = df.apply(lambda row:\n",
    "                                                    get_mean_from_climatestotravel_in_period(start_date_period = row['target_start_date_period'], \n",
    "                                                                                             end_date_period = row['target_end_date_period'], \n",
    "                                                                                             climatestotravel_selected_col = \"average_sunshine_hours_in_month\"),axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "17+19+19/3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['target_start_date_period', 'target_end_date_period', 'start_date_previous_period','end_date_previous_period']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sunshine_hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def from_col_list_to_is_exists_columns(_df, col_of_list , ignore_values_list = ['a','']):\n",
    "    # itreate on all rows and get all optional values in col_of_list\n",
    "    # create list of unique values.\n",
    "    # create df of col_of_list without None\n",
    "    df_col = _df[pd.isna(_df[col_of_list]) == False ][col_of_list]\n",
    "   \n",
    "    merged = list(itertools.chain(*df_col.to_list())) \n",
    "    #display(df_col)\n",
    "    # each row in df_col conatins list of strs, flat all list to single list\n",
    "    flat_list = list(itertools.chain(*df_col.to_list())) \n",
    "    # print(flat_list)\n",
    "    list_of_columns_to_add = list(set(flat_list)-set(ignore_values_list))\n",
    "    # add column be each value of the list (equivalent for creating one hot encoding/dummies)\n",
    "    \n",
    "    for new_col in list_of_columns_to_add:\n",
    "        is_exists = new_col in _df[col_of_list]\n",
    "        \n",
    "        _df[new_col] = _df[col_of_list].apply(lambda x: 1  if (x and new_col in x) else 0 )\n",
    "    \n",
    "    return _df\n",
    "        \n",
    "######################################################################\n",
    "# Testing\n",
    "######################################################################        \n",
    "df_test = pd.DataFrame({'amenities_list': [[\"tv\", \"cable_tv\"],[\"heating\", \"washer\"]] } )\n",
    "df_test = from_col_list_to_is_exists_columns(df_test, col_of_list = 'amenities_list')\n",
    "\n",
    "df_expected = pd.DataFrame({'amenities_list': [[\"tv\", \"cable_tv\"],[\"heating\", \"washer\"]], \"tv\": [1,0], \"cable_tv\":[1,0], \"heating\":[0,1], \"washer\": [0,1] })\n",
    "pd.testing.assert_frame_equal(df_test,df_expected[df_test.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform \"Amenities\" column (set of attributes) column to Dummies columns\n",
    "Amenities columns contains dict of attribute. Transform each attribute to feature.\n",
    "From all attributes that appears in amenities, creating multiple features (equivalent to making dummies/One hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['amenities'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_to_list(amenities_val):\n",
    "    # amenities_val is set of words that is kept in str. Example '{TV,\"Cable TV\",Wifi,Kitchen,Gym, ... }')\n",
    "    amenities_str = str(amenities_val)[1:-1].split(\",\") # ['TV', '\"Cable TV\"', 'Wifi', 'Kitchen', 'Gym']\n",
    "    \n",
    "    # remove \"\" from prases with spaces, replace spaces with _ and cast prases to lower\n",
    "    return [s.strip('\"').lstrip().lower().replace(\" \", \"_\") for s in amenities_str]\n",
    "\n",
    "\n",
    "# Test set_to_list method\n",
    "df_test = pd.DataFrame({'amenities': ['{TV,\"Cable TV\",Wifi,Kitchen,Gym}', '{Heating,Washer,Essentials,Shampoo,\"Hair dryer\"}']} )\n",
    "df_test['amenities_list'] = df_test['amenities'].apply(set_to_list)\n",
    "\n",
    "df_expected = pd.DataFrame({'amenities': ['{TV,\"Cable TV\",Wifi,Kitchen,Gym}', '{Heating,Washer,Essentials,Shampoo,\"Hair dryer\"}'],\n",
    "                           'amenities_list': [[\"tv\", \"cable_tv\", \"wifi\", \"kitchen\", \"gym\"],[\"heating\", \"washer\", \"essentials\", \"shampoo\", \"hair_dryer\"]] } )\n",
    "\n",
    "pd.testing.assert_frame_equal(df_test,df_expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['amenities_list'] = df['amenities'].apply(set_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = from_col_list_to_is_exists_columns(df, col_of_list = 'amenities_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform \"host_verifications\" column (set of attributes) column to Dummies columns \n",
    "host_verifications columns contains list of attribute. Transform each attribute to feature.\n",
    "From all attributes that appears in host_verifications, creating multiple features (equivalent to making dummies/One hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host_verifications'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# host_verifications conatins list that transformed to str (repersented as string)\n",
    "# ast.literal_eval can be use to transform back to list (from the str of list)\n",
    "ast.literal_eval(df['host_verifications'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_verifications_list is the represntaion of host_verifications as list (intead of str)\n",
    "df['host_verifications_list'] = df['host_verifications'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = from_col_list_to_is_exists_columns(df, col_of_list = 'host_verifications_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform categorical variables - dummy encoding\n",
    "add dummies encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Defining the categorical variables:\n",
    "category_cols = ['neighbourhood_group','room_type', 'property_type'\n",
    "'host_response_time','host_is_superhost','host_has_profile_pic',\n",
    "'host_identity_verified', 'bed_type', 'instant_bookable','is_business_travel_ready','require_guest_profile_picture',\n",
    " 'require_guest_phone_verification','cancellation_policy', 'concat_comments_sentiment']\n",
    "# add the numeric categories that transfored to categories colums\n",
    "category_cols += [x for x in df.columns.to_list() if x.endswith(\"cat\")]\n",
    "\n",
    "print(category_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummies = pd.get_dummies(df, columns=category_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_coulmns = list(set(df_with_dummies.columns)-set(df.columns))\n",
    "print(f\"get_dummies method add {len(dummies_coulmns)} columns. The new columns are: \")\n",
    "dummies_coulmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing the dimensionality of the DataFrame (after Feature Enrichment):\n",
    "df_with_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_with_dummies.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_vars = [listing_id, target_start_date_period, target_end_date_period , start_date_previous_period\n",
    ",end_date_previous_period, square_feet, monthly_price, concat_comments_sentiment,target_num_of_booked_days,\n",
    "booked_up_target, num_of_day_in_previous_period , num_of_booked_days_in_previous_period, price,\n",
    "minimum_nights, number_of_reviews, DaysPassed_last_review, calculated_host_listings_count, availability_365,\n",
    "DaysPassed_host_since\n",
    "accommodates\n",
    "bathrooms\n",
    "bedrooms\n",
    "beds\n",
    "security_deposit\n",
    "cleaning_fee\n",
    "guests_included\n",
    "extra_people\n",
    "maximum_nights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "neighbourhood_group\n",
    "neighbourhood\n",
    "latitude\n",
    "longitude\n",
    "room_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['accommodates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Selection based on voting: using many of the techniques (univariate and multivariate), we make\n",
    "a table with all the variables on the dataset and indicate the recommended variables for each\n",
    "technique, then we select a threshold for the total votings and on this basis we select the variables\n",
    "that will be used to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
