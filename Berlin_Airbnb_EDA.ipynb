{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I perform an analysis of the detailed Berlin listings data.\n",
    "I am using visualizing and analyzing data to extract insights from the variables in the data. \n",
    "\n",
    "In the beginning, I use descriptive statistics to explore the data, which can help describe the data set's basic features and obtain a summary of the data. \n",
    "Then, I perform Data Visualization analysis to provide an accessible way to see and understand trends, outliers, relationships, variability, patterns in data and to notice if there is a problem with data quality.\n",
    "\n",
    "I intend to focus on the correlations and the differences between the variables in the data set and describe the target variable \"booked_up_target\", his distribution and its relationships with the variables. \n",
    "\n",
    "Exploratory data analysis also helps us deriving new variables or perform variable transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining and Viewing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_dict_of_df_types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-28aa9187824b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmissingno\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mget_dict_of_df_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_dict_of_df_types' is not defined"
     ]
    }
   ],
   "source": [
    "# Import libraries:\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matploget_dict_of_df_typestlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import folium as fl\n",
    "from folium.plugins import FastMarkerCluster\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import missingno as msno\n",
    "get_dict_of_df_types\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matploget_dict_of_df_typestlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ODBC:\n",
    "\"\"\"\n",
    "driver_name = \"SQL Server Native Client 11.0\"\n",
    "server = \"ITZIK\\SQLEXPRESS\"\n",
    "dbname = \"Berlin\"\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding the output display to see more rows and columns:\n",
    "pd.set_option('display.max_rows', 200 , 'display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data:\n",
    "\"\"\"\n",
    "conn = pyodbc.connect('Driver={};'\n",
    "                      'Server={};'\n",
    "                      'Database={};'\n",
    "                      'Trusted_Connection=yes;'.format(driver_name, server, dbname))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_sql_query('SELECT * FROM db01.FINAL_TABLE',conn)\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"flat_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the first 3 rows of dataframe using head() method:\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from sql add some additional index columns that starts with \"Unamed\" - dropping this columns\n",
    "columns_to_drop = [x for x in df.columns.to_list() if x.startswith(\"Unnamed\")]\n",
    "print(\"dropping coulmns: \", columns_to_drop) # [Unamed..., Unamed..]\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify listing id and host id are int type\n",
    "df[\"listing_id\"] = df[\"listing_id\"].astype('int')\n",
    "df[\"host_id\"] = df[\"host_id\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing the dimensionality of the DataFrame (before adding new variables- after the EDA):\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Displaying description of mean, standard deviation, quartiles and maximum & minimum values:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema of DataFrame:\n",
    "# list(df.columns.values)\n",
    "df.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of missing values in the data set\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying variables cleaning_fee, extra_people, security_deposit\n",
    "df[['cleaning_fee', 'extra_people', 'security_deposit']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For exploration, removing the \"$\"-Sign and formatting as float of vars cleaning_fee, extra_people, security_deposit\n",
    "\n",
    "df.cleaning_fee.fillna('$0.00', inplace=True)\n",
    "df.extra_people.fillna('$0.00', inplace=True)\n",
    "df.security_deposit.fillna('$0.00', inplace=True)\n",
    "df.cleaning_fee = df.cleaning_fee.str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df.security_deposit = df.security_deposit.str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df.extra_people = df.extra_people.str.replace('$', '').str.replace(',', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed!\n",
    "df[['cleaning_fee', 'extra_people', 'security_deposit']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange the columns by type\n",
    "\n",
    "def get_dict_of_df_types(pdf: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Arrange dataframe columns in the dictionary by types:\n",
    "    For each type having a list of columns\n",
    "    \"\"\"\n",
    "    d_of_columns_types = {}\n",
    "\n",
    "    for c,t in zip(pdf.columns, pdf.dtypes):\n",
    "        t_str = str(t)\n",
    "        if d_of_columns_types.get(t_str) == None:\n",
    "            d_of_columns_types[t_str] = [c]\n",
    "        else:\n",
    "            d_of_columns_types[t_str].append(c)\n",
    "    return d_of_columns_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_of_columns_types_local = get_dict_of_df_types(df)\n",
    "d_of_columns_types_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Defining the categorical variables:\n",
    "category_cols = ['neighbourhood_group','room_type',\n",
    "'host_response_time','host_is_superhost','host_has_profile_pic',\n",
    "'host_identity_verified', 'bed_type', 'instant_bookable','is_business_travel_ready','require_guest_profile_picture',\n",
    " 'require_guest_phone_verification','cancellation_policy', 'concat_comments_sentiment']\n",
    "\n",
    "# verify category cols are defined as category\n",
    "for col in category_cols:\n",
    "    df[col] = df[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_of_columns_types_local = get_dict_of_df_types(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(d_of_columns_types_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequncy values in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating table of frequencies values for each category.\n",
    "# The bellow tables would be helpful in the feature engineering and feature selection satges.\n",
    "\n",
    "category_values = {}\n",
    "category_dfs = {}\n",
    "for x in d_of_columns_types_local['category']:\n",
    "    category_values[x] = df[x].value_counts().to_dict()\n",
    "    category_value_list = list(category_values[x].keys())\n",
    "    category_count_list = list(category_values[x].values())\n",
    "    category_dfs[x] = pd.DataFrame({x: category_value_list, \"count\":category_count_list })\n",
    "    display( category_dfs[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of the numeric variables:\n",
    "columns_to_drop = ['listing_id', 'host_id', 'zipcode', 'host_acceptance_rate','xl_picture_url','medium_url', 'thumbnail_url','jurisdiction_names','news_id']\n",
    "\n",
    "columns_to_drop = [col_name for col_name in columns_to_drop if col_name in df.columns]\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=False).hist(bins=30, figsize=(35, 35))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each listing_id has 7 records in df. \n",
    "# For ploating the location at the property type, each propetry should appear only once. \n",
    "\n",
    "# selecting colums that desribe the property/listening \n",
    "df_unique_listings = df[['listing_id',  'name', 'host_id','host_name','neighbourhood_group', 'neighbourhood', 'latitude','longitude', 'room_type','property_type', 'minimum_nights', 'cancellation_policy','host_response_time', 'bathrooms', 'bedrooms', 'accommodates', 'security_deposit', 'cleaning_fee', 'review_scores_communication']]\n",
    "# Drop Duplicates (the descriptions coulmns of listening id are duplicate 7 times, we need them only once)\n",
    "df_unique_listings = df_unique_listings.drop_duplicates('listing_id')\n",
    "\n",
    "print(\"df num of records:\", len(df))\n",
    "print(\"df_unique_listings num of records: \", len(df_unique_listings), \"Each listing_id appears:\", len(df)/len(df_unique_listings))\n",
    "df_unique_listings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map landmarks by properties:\n",
    "latitude_values = df_unique_listings['latitude'].to_list()\n",
    "longitude_values =  df_unique_listings['longitude'].to_list()\n",
    "locations = list(zip(latitude_values, longitude_values))\n",
    "\n",
    "#values passed in the parameters of Map function are the latitute and longitude of Berlin\n",
    "finalMap = fl.Map(location=[52.5200, 13.4050], zoom_start=12)\n",
    "FastMarkerCluster(data=locations).add_to(finalMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each number in the bellow map is the number of a group of propeties \n",
    "# Using Scroll down and up you can view the location of the properties (Cliclk on the numbers for drill down to the property).\n",
    "finalMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting map of Berlin by neighbourhood\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.set_style('white')\n",
    "customPalette = ['#800000', '#e6194B', '#f58231', '#ffe119', '#3cb44b', '#42d4f4', '#911eb4', '#000000', '#000075', '#444444', '#008080', '#ec0101']\n",
    "sns.scatterplot(x=df_unique_listings['latitude'], y=df_unique_listings['longitude'],hue=df_unique_listings[\"neighbourhood_group\"], palette=sns.set_palette(customPalette))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of properties in each neighbourhood_group\n",
    "# Friedrichshain-Kreuzberg, Mitte Pankow and Neuk��lln are the most common neighbourhood group for Air-bnb in Berlin.\n",
    "df_neighbourhood_group = df_unique_listings['neighbourhood_group'].value_counts()\n",
    "df_neighbourhood_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot of neighbourhood_group\n",
    "plt.figure(figsize=(30,10))\n",
    "sns.barplot(x=df_neighbourhood_group.index, y=df_neighbourhood_group ,palette=sns.color_palette('magma', n_colors=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property type and room type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property type deployment - TOP-10 types\n",
    "# Significantly most of the property type in Berlin is an apartment.\n",
    "plt.figure(figsize = (15,5))\n",
    "sns.countplot(df_unique_listings['property_type'], order = df_unique_listings.property_type.value_counts().iloc[:10].index)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Quantity of listings\", fontsize = 'large')\n",
    "plt.title(\"Property type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room type deployment\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.countplot(df_unique_listings['room_type'], order = df_unique_listings.room_type.value_counts(normalize = True).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Host response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like a good percentage of hosts respond within an hour.\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(df_unique_listings['host_response_time'], order = df_unique_listings.host_response_time.value_counts(normalize = True).index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minimum_nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are outliers in the variable \"minimum nights\" \n",
    "# (there are values that are reasonable as minimum nights can, for example, 5k as minimum nights) \n",
    "# we will need to take care of this variable in the Data Cleansing section.\n",
    "# In most cases the requirement for minimum nights is low.\n",
    "\n",
    "print(df_unique_listings[\"minimum_nights\"].describe())\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.countplot(x=\"minimum_nights\", data=df_unique_listings)\n",
    "plt.xticks(rotation=1000)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values of bathrooms must be integer values and not float such as 8.5 \n",
    "# Also I need to understand whether it is resonable that property can have 0 or more than 4 bathrooms \n",
    "print(df_unique_listings['bathrooms'].describe())\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.displot(data=df_unique_listings , x=\"bathrooms\")\n",
    "plt.xticks(rotation=1000)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check whether the following is resonable\n",
    "# is 0 bedrooms resnoable ? - is it studio property?\n",
    "# is 12 bedrooms resnoable? - is it releated to the property that has 8 bathrooms?\n",
    "\n",
    "print(df_unique_listings[\"bedrooms\"].describe())\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.displot(data=df_unique_listings , x=\"bedrooms\")\n",
    "plt.xticks(rotation=1000)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cancellation_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In most cases the cancellation policy is flexible.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.displot(data=df_unique_listings , x=\"cancellation_policy\")\n",
    "plt.xticks(rotation=1000)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review_scores_communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the values review_scores_communication in the range between 0 to 10.\n",
    "# the bellow seems reasonable, so no need to handle this in the data cleansing stage\n",
    "print(df_unique_listings[\"review_scores_communication\"].describe())\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.displot(data=df_unique_listings , x=\"review_scores_communication\")\n",
    "plt.xticks(rotation=1000)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Availability / Occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the occupancy perecantge in the last period (the period before the target)\n",
    "# the bellow seems reasonable, so no need to handle this in the data cleansing stage\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(data=df.occupancy_last_period, shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are outliers in the variable \"number_of_reviews\".\n",
    "# we can assume that outliers are resnoable, there are some properies with many reviews. \n",
    "print(df['number_of_reviews'].describe())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(data=df.number_of_reviews, shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review_scores_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are outliers in the variable \"review_scores_rating\".\n",
    "# The review score rating are in range 0 to 100 (no need to handle this in data cleansing section)  \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(data=df.review_scores_rating, shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some records with not reasonable price for a night, \n",
    "# we need to check whether a price of 9K is reasonable per night and if not fixing it as outliers\n",
    "print(df['price'].describe())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(data=df.price, shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average dollar price in the target period\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(data=df.target_avg_dollar_price_in_period, shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average dollar price in the previous period\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(data=df.avg_dollar_price_in_previous_period, shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### security_deposit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems there are outliers in the security deposit that need to be handle.\n",
    "\n",
    "print(df['security_deposit'].describe())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(data=df.security_deposit, shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning_fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems there are outliers in the cleaning fee that need to be handle in data cleasing stage\n",
    "\n",
    "print(df['cleaning_fee'].describe())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(data=df.cleaning_fee, shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### amentities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amentities are list of strings, in feature engineering I will cast each string to catgeory\n",
    "# (and using dummies or one hot encoding)\n",
    "df['amenities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### host_verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_verifications are list of strings, in feature engineering I will cast each string to catgeory \n",
    "# (and using dummies or one hot encoding)\n",
    "\n",
    "df['host_verifications']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I explore the relationship between the continuous variables. \n",
    "\n",
    "The correlation coefficient can range from -1 to +1, which signifies a strong negative to a strong positive relationship between the variables.\n",
    "Though correlation analysis helps us in understanding the association between two variables in a dataset, it can't explain, or measure, the cause.\n",
    "\n",
    "The p-value helps to determine the significance of the results, when p-value<0.05 it means that the correlation is significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corr = df.corr(method='spearman')\n",
    "corr.style.background_gradient(cmap='coolwarm', axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatt = df.corr(method='spearman')\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(70,60)\n",
    "sns.heatmap(corrMatt, mask=mask,vmax=0.8, square=True,annot=True, annot_kws={'size':25})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two functions :\n",
    "# 1- my_spearmanr- Spearman's rank correlation coefficient test, which is a nonparametric measure of rank correlation\n",
    "# 2- all_numric_correlations - list of all the combinations numeric correlations and their p-value\n",
    "    \n",
    "\n",
    "def my_spearmanr(df, x1, x2):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df_sub = df[[x1,x2]].dropna(axis=0, inplace=False)\n",
    "    return stats.spearmanr(df_sub[x1],df_sub[x2])\n",
    "   \n",
    "\n",
    "def all_numric_correlations(df, numeric_columns_list, max_pvalue_threshold=0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    res = list(combinations(['a' ,'b', 'c'], 2))\n",
    "    res = [('a', 'b'), ('a', 'c'), ('b', 'c')]\n",
    "    \"\"\"\n",
    "    numeric_combinations_list = list(combinations(numeric_columns_list, 2))\n",
    "    results_list = []\n",
    "    for col1, col2 in numeric_combinations_list:\n",
    "        my_spearmanr_res = my_spearmanr(df, col1, col2)\n",
    "        pvalue=my_spearmanr_res[1]\n",
    "        if pvalue < max_pvalue_threshold:\n",
    "            results_list.append({\"var1\": col1, \"var2\": col2, \"correlation\": my_spearmanr_res[0], \"pvalue\": my_spearmanr_res[1]})\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list).sort_values(by='correlation', ascending=False)\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "numeric_columns_list = d_of_columns_types_local['float64'] + d_of_columns_types_local['int64']\n",
    "\n",
    "df_numric_correlations = all_numric_correlations(df,numeric_columns_list)\n",
    "df_numric_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see relationship between some variables in graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='avg_dollar_price_in_previous_period', y='target_avg_dollar_price_in_period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='availability_60', y='availability_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='target_avg_dollar_price_in_period', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='avg_dollar_price_in_previous_period', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='calculated_host_listings_count', y='host_total_listings_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='number_of_reviews', y='reviews_per_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='accommodates', y='beds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='occupancy_last_period', y='DaysPassed_last_review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## booked_up - Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I examine the target variable - \"booked_up_target\",  which is a categorical variable.\n",
    "To explore how the variable distributes and its relationships with the variables.\n",
    "\n",
    "I examine whether there are numerical or categorical variables that show a significant difference in the distribution of the target variable- \"booked_up_target\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable distribution\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"booked_up_target\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_booked_up_target_summary = df[\"booked_up_target\"].value_counts().to_frame()\n",
    "df_booked_up_target_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot = df_booked_up_target_summary.plot.pie(y='booked_up_target', title=\"booked up target summary\", legend=False, \\\n",
    "                   autopct='%1.1f%%', \\\n",
    "                   shadow=True, startangle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see our numeric variables so far\n",
    "numeric_list= d_of_columns_types_local['int64'] + d_of_columns_types_local['float64']\n",
    "numeric_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the relationships between the numeric variables and the target with barplot\n",
    "for y in numeric_list:\n",
    "    plt.figure()\n",
    "    sns.barplot(x=\"booked_up_target\", y=y, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the relationships between the numeric variables and the target with boxplot\n",
    "for y in numeric_list:\n",
    "    plt.figure()\n",
    "    sns.catplot(x=\"booked_up_target\", y=y, kind=\"box\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the relationships between the categorical variables and the target \n",
    "# in order to check if the outcome is affected by any categorical variable\n",
    "for y in d_of_columns_types_local['category']:\n",
    "    plt.figure(figsize = (30,10))\n",
    "    sns.factorplot(x =y, y =\"booked_up_target\", data = df, kind ='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mann–Whitney U test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, the Mann–Whitney U test is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations. \n",
    "\n",
    "The goal is to find out if there are any differences between the target variable - \"booked_up_target\" (category) and the other numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two functions :\n",
    "# 1- my_mannwhitneyu: for Mann–Whitney U test.\n",
    "# 2- all_combination_mannwhitneyu: Creating the Dataframe of the results for the test, when p-value<0.05 (only the significance differences). \n",
    "def my_mannwhitneyu(df, x, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df_sub = df[[x,y]].dropna(axis=0, inplace=False)\n",
    "    return stats.mannwhitneyu(df_sub[x],df_sub[y])\n",
    "\n",
    "\n",
    "def all_combination_mannwhitneyu(df, numeric_columns_list, y_target=\"booked_up_target\", max_pvalue_threshold=0.05):\n",
    "    results_list = []\n",
    "    for num_col in numeric_columns_list:\n",
    "        mannwhitneyu_res = my_mannwhitneyu(df, num_col, y_target)\n",
    "        pvalue=mannwhitneyu_res[1]\n",
    "        if pvalue < max_pvalue_threshold:\n",
    "            res = {\"x\": num_col, \"y\":y_target,\"statistic\":mannwhitneyu_res[0], \"pvalue\":mannwhitneyu_res[1]}\n",
    "        results_list.append(res)\n",
    "    df_results = pd.DataFrame(results_list).sort_values(by='statistic', ascending=False)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with only the variables that have significant differences to the target variable\n",
    "numeric_columns_list = d_of_columns_types_local['float64'] + d_of_columns_types_local['int64']\n",
    "\n",
    "df_categories_mannwhitneyu = all_combination_mannwhitneyu(df, numeric_columns_list, \"booked_up_target\")\n",
    "df_categories_mannwhitneyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I examine if there is anomalies in the data in order to check if there are outliers in the variables. \n",
    "\n",
    "Outliers are extreme values that deviate from other observations on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an ID variable\n",
    "df['news_id'] = [i for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in numeric_list:\n",
    "    plt.figure()\n",
    "    sns.scatterplot(data=df, x='news_id', y=x, palette=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there are variables with outliers like price, minimum nights, reviews per month and etc that we will need to treat them in the data cleansing section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the Missing Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting columns that have Null(s)\n",
    "missing_data_cols_names_list = df.columns[df.isnull().any()].tolist()\n",
    "msno.matrix(df[missing_data_cols_names_list])\n",
    "print(\"number of columns that have Nulls: \", len(missing_data_cols_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As ploting above, becuase \"missing_data_cols_names_list\" has many columns \n",
    "# and I can't see columns names I split to chucnk of columns only for view the column names.\n",
    "n=15 # max number of columns in each chunk\n",
    "for i in range(0, len(missing_data_cols_names_list), n):\n",
    "    start_chunk_index = i\n",
    "    end_chunk_index = i+n if i+n < len(missing_data_cols_names_list) else len(missing_data_cols_names_list)\n",
    "    print(\"selecting columns in indexes [\", start_chunk_index, \": \", end_chunk_index, \"]\")\n",
    "    msno.matrix(df[missing_data_cols_names_list[i:i+n]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there are variables with few, many or that the whole column is missing. We will need to treat them in the data cleansing section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## missingness correlation heatmap\n",
    "\n",
    "msno.heatmap(df[missing_data_cols_names_list], figsize=(30,30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
